{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44d1d12",
   "metadata": {},
   "source": [
    "#### Preprocessing Data\n",
    "\n",
    "the preprocessing step\n",
    "\n",
    "all the data we've been using so far has been pretty nice, it's been in a format that's allowed you to plug and play inte scikit-learn without any extra processing, but that won't be the case in the real world, you'll have to preprocess your data before you can build models \n",
    "\n",
    "scikit-learn api will not accept categorical features like \"male\" and \"female\" so you'll have to encode them numerically, you can do this by splitting the feature into a number of binary features called dummy variables with 0 meaning the observation wasn't that category and 1 meaning it was, if you have 3 colors like red, blue, and purple, you only need 0 and 1 because if both blue and red are 0 you implicitly know that the color is purple\n",
    "\n",
    "boxplots are useful for visualizing categorical features\n",
    "\n",
    "you could use scikit-learn's OneHotEncoder() or pandas' get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ee88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding how mpg varies by US, ASIA, and EUROPE using dummy variables\n",
    "import pandas as pd\n",
    "\n",
    "# read in the dataframe \n",
    "df = pd.read_csv('auto.csv')\n",
    "# apply the get_dummies() function\n",
    "df_origin = pd.get_dummies(df)\n",
    "\n",
    "print(df_origin.head())\n",
    "# this will create 3 new binary features, looking at them you know if Europe and US are both 0 that the car's origin is Asia\n",
    "# that 3rd column is redundant info so we can drop it\n",
    "\n",
    "# drop the redundant column\n",
    "df_origin = df_origin.drop('origin_Asia', axis=1)\n",
    "print(df_origin.head())\n",
    "# another option to do this is to pass the drop first option to get_dummies: df_region = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# now that you have the dummy variables you can fit models just like before\n",
    "# fit the ridge regression model to the data and compute its r squared\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "ridge = Ridge(alpha=0.5, normalize=True).fit(X_train, y_train)\n",
    "\n",
    "ridge.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b04d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, exploring categorical features using a boxplot\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read 'gapminder.csv' into a DataFrame: df\n",
    "df = pd.read_csv('gapminder.csv')\n",
    "\n",
    "# Create a boxplot of life expectancy per region\n",
    "df.boxplot('life', 'Region', rot=60)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ef452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, ridge regression with categorical features\n",
    "# Import necessary modules\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate a ridge regressor: ridge\n",
    "ridge = Ridge(alpha=0.5, normalize=True)\n",
    "\n",
    "# Perform 5-fold cross-validation: ridge_cv\n",
    "ridge_cv = cross_val_score(ridge, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validated scores\n",
    "print(ridge_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9677eef7",
   "metadata": {},
   "source": [
    "#### Handling Missing Data\n",
    "\n",
    "missing data can exist because of a lack of observation, a transcription error, corrupted data, etc.\n",
    "\n",
    "sometimes you'll look at the info of a dataframe and all the features will have the correct number of non-null entries but you have to remember that missing values can be encoded in a bunch of different ways (like zeroes, question marks, negatives), for example, if insulin, BMI, or thickness of skin are 0 that doesn't make sense since that's not possible\n",
    "\n",
    "you can make all those weird 0 entries NaN by using replace() or the relevant columns\n",
    "to deal with missing data you could drop all rows that are missing data but if you lose half the data that is bad \n",
    "\n",
    "a more robust option could be to **impute** (make an educated guess about) missing data, you could compute the mean of the all non-missing entries and then fill that in for all the missing ones\n",
    "imputers are also known as transformers, any model that can transform data like the example below (using the transform method) is called a transformer\n",
    "\n",
    "after you transform the data you could then fit a supervised learning model to it, you can do both at once using a scikit-learn pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the dataset\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "df.info()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b005f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping missing data\n",
    "df.insulin.replace(0, np.nan, inplace=True)\n",
    "df.triceps.replace(0, np.nan, inplace=True)\n",
    "df.bmi.replace(0, np.nan, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3391ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows with missing data\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing data\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# the missing values are represented by NaN, strategy is to use the mean, axis=0 means it'll impute along coulmns (1=rows)\n",
    "imp=Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "\n",
    "# fit the imputer to the data\n",
    "imp.fit(X)\n",
    "\n",
    "# transform the data\n",
    "X = imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing within a pipeline\n",
    "from sklearn.pipeline import PipeLine\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "# instantiate a logreg model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# build the pipeline object, construct a list of steps\n",
    "# each step is a 2-tuple containing the name for the step and the estimator \n",
    "# all the steps except the last have to be a transformer, the last step has to be an estimator like a classifier or regressor \n",
    "steps = [('imputation', imp), ('logistic_regression', logreg)]\n",
    "# pass the list to the pipeline constructor\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# split the data into training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# fit the pipeline to the training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "# predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# compute accuracy\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086efdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, replace ? with NaN and then drop those suckers from the datafram\n",
    "# Convert '?' to NaN\n",
    "df[df == '?'] = np.nan\n",
    "\n",
    "# Print the number of NaNs\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Print shape of original DataFrame\n",
    "print(\"Shape of Original DataFrame: {}\".format(df.shape))\n",
    "\n",
    "# Drop missing values and print shape of new DataFrame\n",
    "df = df.dropna()\n",
    "\n",
    "# Print shape of new DataFrame\n",
    "print(\"Shape of DataFrame After Dropping All Rows with Missing Values: {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8fafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, imputing mode with a pipeline, with a Support Vector Machine classifier\n",
    "# Import the Imputer module\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.svm import SVC #support vector classification, a type of SVM\n",
    "\n",
    "# Setup the Imputation transformer: imp\n",
    "imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "\n",
    "# Instantiate the SVC classifier: clf\n",
    "clf = SVC()\n",
    "\n",
    "# Setup the pipeline with the required steps: steps\n",
    "steps = [('imputation', imp),\n",
    "        ('SVM', clf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea13eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, imputing missing data by using the pipeline interface and create a classification report\n",
    "# Import necessary modules\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [('imputation', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),\n",
    "        ('SVM', SVC())]\n",
    "\n",
    "# Create the pipeline: pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit the pipeline to the train set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc5fab2",
   "metadata": {},
   "source": [
    "#### Centering and Scaling\n",
    "\n",
    "centering and scaling (normalizing) your data is another important preprocessing step for machine learning\n",
    "\n",
    "you can use df.describe to check out the ranges of the feature variables, many ml models use some form of distance to inform them so if you have features on really large scales the could unduly influence the model, one example is that knn explicitly uses distance when making predictions and because of that we want features to be on a similar scale\n",
    "\n",
    "scaling with have minimal effect when all the features are binary though :(\n",
    "\n",
    "ways to normalize data:\n",
    "- **standardization** for a column, subtract the mean and divide by the variance so that all the features are centered around 0 and have a variance of 1\n",
    "- you could subtract the minimum and divide by the range of the data so the normalized dataset has minimum 0 and maximum 1\n",
    "- normalize so that the data ranges from -1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143681a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the ranges of the feature variables\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale with scikit-learn\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# pass the feature data to the scale() method\n",
    "X_scaled = scale(X)\n",
    "\n",
    "# you can compare the mean and st dev of the columns of the original and scaled data to see the change\n",
    "np.mean(X), np.std(X) # original\n",
    "np.mean(X_scaled), np.std(X_scaled) # scaled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4694ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also but a scaler in a pipeline object\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "steps =[('scaler', StandardScaler()), ('knn', KNeighborClassifier())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "knn_scaled = pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred) # you get .956\n",
    "\n",
    "# performing knn without scaling\n",
    "knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n",
    "knn_unscaled.score(X_test, y_test) # you get .928 which isn't as good as the scaled data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec6efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV and scaling in a pipeline\n",
    "\n",
    "steps =[('scaler', StandardScaler()), ('knn', KNeighborClassifier())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# specify the hyperparameter space by creating a dictionary\n",
    "# the keys are pipeline step name followed by a double underscore, followed by the hyperparameter name\n",
    "# the corresponding value is an list or array of the values to try for that particular hyperparameter\n",
    "# in this example we're only tuning the n neighbors in the KNN model\n",
    "parameters = {knn__n_neighbors: np.arange(1, 50)}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "\n",
    "# perform a grid search over the parameters pipeline\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# predict on the estimator with the best found parameters and we do this on the holdout set\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "# print the best parameters, accuracy(score) , and classification report\n",
    "print(sv.best_params_)\n",
    "print(cv.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, the whole shebang\n",
    "#  build a pipeline that includes scaling and hyperparameter tuning to classify wine quality\n",
    "\n",
    "# Setup the pipeline\n",
    "steps = [('scaler', StandardScaler()),\n",
    "         ('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {'SVM__C':[1, 10, 100],\n",
    "              'SVM__gamma':[0.1, 0.01]}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "\n",
    "# Instantiate the GridSearchCV object, cv,  with the pipeline and hyperparameter space with 3 fold cross validation, 3 is the default so you don't see it here\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "# Fit to the training set\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, the whole shebang again\n",
    "# build a pipeline that imputes the missing data, scales the features, and fits an ElasticNet to the Gapminder data\n",
    "# then tune the l1_ratio of your ElasticNet using GridSearchCV\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "# impute the missing data, scale the features, instantiate an elastic net regressor\n",
    "steps = [('imputation', Imputer(missing_values='NaN', strategy='mean', axis=0)),\n",
    "         ('scaler', StandardScaler()),\n",
    "         ('elasticnet', ElasticNet())]\n",
    "\n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "# Fit to the training set\n",
    "gm_cv.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print the metrics\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "print(\"Tuned ElasticNet Alpha: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
