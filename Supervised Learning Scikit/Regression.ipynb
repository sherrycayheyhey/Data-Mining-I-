{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40695e01",
   "metadata": {},
   "source": [
    "#### Intro to Regression\n",
    "\n",
    "regression is another type of supervised learning problem (supervised learning/classification was the one we just learned)\n",
    "\n",
    "in regression tasks the target variable is a continuously varying variable, such as GDP or the price of a house \n",
    "\n",
    "if the target variable is quantitative then it's a regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from a csv using pandas' read_csv() function\n",
    "boston = pd.read_csv('boston.csv')\n",
    "# and view the head\n",
    "print(boston.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e196e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn wants features and target values in distinct arrays, X and y, so split the dataframe\n",
    "# the .values attribute return the NumPy arrays that we'll use\n",
    "X = boston.drop('MEDV', axis=1).values # drop the target\n",
    "y = boston['MEDV'].values # keep only the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5232cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting house value from a single feature, the average number of rooms in a block\n",
    "X_rooms = X[:, 5] # slice out the rooms column (the 5th column in this case)\n",
    "# check the types and you'll see that both and NumPy arrays\n",
    "type(X_rooms), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the NumPy arrays into the desired shape, keep the first dimension but add another dimension of size 1 to X\n",
    "y = y.reshape(-1, 1)\n",
    "X_rooms = X_rooms.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot house value as a function of number of rooms\n",
    "plt.scatter(X_rooms, y)\n",
    "plt.ylabel('Value of house /1000 ($)') # label the x axis\n",
    "plt.xlabel('Number of rooms') # label the y axis\n",
    "plt.show();\n",
    "# you would see that, as expected, more rooms leads to higher prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff1650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now fit a regression model to the data\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate LinearRegression as reg\n",
    "reg = LinearRegression()\n",
    "# fit the regressor to the data\n",
    "reg.fit(X_rooms, y\n",
    "# check out the regressor's predictions over the range of the data\n",
    "prediction_space = np.linspace(min(X_rooms), max(X_rooms)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the line with a scatter plot\n",
    "plt.scatter(X_rooms, y, color='blue')\n",
    "plt.plot(prediction_space, reg.predict(prediction_space), color='black', linewidth=3)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f089d20",
   "metadata": {},
   "source": [
    "#### The Basics of Linear Regression\n",
    "\n",
    "in linear regression you want to fit a line to the data, a line in two dimensions is always in the form y=ax+b:\n",
    "- y is the target\n",
    "- x is the single feature\n",
    "- a and b are the parameters of the model that we want to learn\n",
    "\n",
    "the question of fitting comes down to how to choose a and b \n",
    "\n",
    "one method is to find an error function (also called a loss or cost function) for any given line and then choose the line that minimizes the error function\n",
    "\n",
    "but what will our loss function be? you want the line to be as close to the actual data points as possible, which means minimizing the vertical distance between the fit and the data, for each data point we calculate the vertical distance between it and the line and this distance is called the residual\n",
    "\n",
    "remember that if you tried to minimize the sum of the residuals then you run into the issue of a large positive residual canceling out a large negative residual, that's why our goal is to minimize the **sum of the squares** of the residuals, this will be our loss function which is also called **ordinary least squares OLS**, this is the same as minimizing the mean squared error of the predictions on the training set, when you call fit on a linear regression model in scikit-learn it performs OLS under the hood \n",
    "\n",
    "if you have 2 features and 1 target, the line will be in the form of y=a~1~x~1~+a~2~x~2~+b, so to fit a linear regression model means you specify 3 variables: a1, a2, and b, even higher dimensions just adds more to the equation and means you have to specify a coefficient (ai) for each features as well as the variable (b)\n",
    "\n",
    "the scikit-learn API workes the same way, you pass two arrays, one containing the features and the other containing the target variable\n",
    "\n",
    "the default scoring method (metric) for linear regression is called R squared, this metric quantifies the amount of variance in the target variable that is predicted from the feature variables \n",
    "\n",
    "you could also compute RMSE (root mean error squared) which is another commonly used metric to evaluate regression models\n",
    "\n",
    "using all features will improve the model score, which makes sense, buuuuuut this will probably lead to overfitting and won't generalize to other data so the next lesson we'll learn how to better evaluate our models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression on all features\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# instantiate the regressor\n",
    "reg_all = LinearRegression()\n",
    "\n",
    "# fit it on the training set\n",
    "reg_all.fit(X_train, y_train)\n",
    "\n",
    "# predict on the test set\n",
    "y_pred = reg_all.predict(X_test)\n",
    "\n",
    "# compute r squared, pass it the test data and the test data target \n",
    "reg_all.score(X_test, y_test)\n",
    "\n",
    "# Plot regression line\n",
    "plt.plot(prediction_space, y_pred, color='black', linewidth=3)\n",
    "plt.show()\n",
    "# you'll almost never use linear regression right out of the box like this but will use regularization\n",
    "# regularization will place further constraints on the model coefficients but this is the first step toward using regularized linear models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49173bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, split the dataset into train and test, predict a linear regression over all features, compute R^2^, \n",
    "# and RMSE (root mean error squared)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5adf4f3",
   "metadata": {},
   "source": [
    "#### Cross Validation\n",
    "\n",
    "you're now more familiar with train test splits and computing model performance metrics on the test set, but there's a potential pitfall of this process\n",
    "you're computing R^2^ on the test set so the result will depend on the way the data was split up, the data in the test set may have weird shit that means the R^2^ computed from it isn't representative of the model's ability to generalize to unseen data \n",
    "to combat this dependence on an artibrary split you can use cross validation \n",
    "\n",
    "cross validation is a vital step in validating a model because it maximizes the amount of data that's used to train the model because the model will be trained and tested on all of the available data\n",
    "\n",
    "**cross validation** \n",
    "- start by splitting the dataset into 5 folds (groups)\n",
    "- hold out the first fold as a test set \n",
    "- fit the  model on the remaining 4 folds \n",
    "- predict on the test set\n",
    "- compute the metric of interest\n",
    "- repeat these steps again but use the second fold as the test set, and again, and again, and again\n",
    "\n",
    "you'll then have 5 values of R^2^ and then you can compute statistics of interest such as the mean, median, and 95% confidence intervals\n",
    "\n",
    "using more folds is more computationally expensive since you're fitting and predicting more times, you could use %timeit right before the cross_val_score() to compare how long something like 10 folds is compared to just 3 folds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f41d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation using scikit-learn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate our model (the regressor)\n",
    "reg = LinearRegression()\n",
    "\n",
    "# call cross_val_score with the regressor, the feature data, the target, and the number of folds\n",
    "cv_results = cross_val_score(reg, X, y, cv=5) \n",
    "# this returns an array of CV scores, the score reported (one for each fold) is R^2^ because\n",
    "# that's the default for linear regression\n",
    "print(cv_results)\n",
    "\n",
    "# compute the mean\n",
    "np.mean(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1874b81f",
   "metadata": {},
   "source": [
    "#### Regularized Regression\n",
    "\n",
    "fitting a linear regression means minimizing a loss function, it chooses a coefficient for each feature variable, if you allow these coefficients/parameters to be super large we'll end up with overfitting, because of this, it's common to use **regularization** to change the loss function so that it penalizes for large coefficients \n",
    "\n",
    "one type of regularization is **ridge regression** which uses the standard OLS loss function plus the squared value of each coefficient multiplied by some constant alpha, thds will make it so coefficients with a large magnitude are penalized, whether positive or negative, alpha is a parameter that we need to choose in order to fit and predict, we'll select the alpha that makes our model perform the best, it's similar to picking K in KNN, this is called hyperparameter tuning and we'll see more of this later, this alpha value is also called **lambda** and can be thought of as a parameter that controls model complexity, when alpha=0 we'll get back OLS which can lead to overfitting because large coefficients aren't penalized and the issue of overfitting isn't accounted for, a really high alpha can mean overpenalization which can lead to a model that's too simple and underfit\n",
    "\n",
    "**lasso regression** is another type of regularized regression, the loss function is the standard OLS loss function plus the absolute value of each coefficient multiplied by some constant alpha, a cool feature of lasso regression is that it can be used to select important features of a dataset, this happens because it tends to shrink the less important features to be exactly 0 then the features that aren't shrunk to 0 are selected by the LASSO algorithm, lasso regression is a great sanity check and is a good way to communicate important result to non-technical colleagues\n",
    "\n",
    "lasso regression is great for feature selection but when building regression models you should make ridge regression your first choice\n",
    "\n",
    "the next thing you'll want to learn is which alpha should you pick? how can you fine tune the model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d0a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge regression in scikit-learn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#fit on the training, predict on the test\n",
    "ridge = Ridge(alpha=0.1, normalize=True) #normalize ensures all the variables are on the same scale\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "ridge.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso regression in scikit-learn\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#fit on the training, predict on the test\n",
    "lasso = Lasso(alpha=0.1, normalize=True) #normalize ensures all the variables are on the same scale\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "lasso.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836bcb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso for feature selection in scikit-learn\n",
    "from sklearn.linear_model import lasso\n",
    "\n",
    "# store the feature names in the variable names\n",
    "names = boston.drop('MEDV', axis=1).columns\n",
    "# instantiate the regressor and fit it to the data\n",
    "lasso = Lasso(alpha=0.1)\n",
    "# extract the coef attribute and store it in a variable\n",
    "lasso_coef = lasso.fit(X, y).coef_\n",
    "# plot the coefficients as a function of feature names to see the figure\n",
    "_ = plt.plot(range(len(names)), lasso_coef)\n",
    "_ = plt.xticks(range(len(names)), names, rotation=60)\n",
    "_ = plt.ylabel('Coefficients')\n",
    "plt.show()\n",
    "# you'll see that the most important predictor for the target variable is the highest point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75771ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example exercise, plot a bunch of alphas\n",
    "# Import necessary modules\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Setup the array of alphas and lists to store scores\n",
    "alpha_space = np.logspace(-4, 0, 50)\n",
    "ridge_scores = []\n",
    "ridge_scores_std = []\n",
    "\n",
    "# Create a ridge regressor: ridge\n",
    "ridge = Ridge(normalize=True)\n",
    "\n",
    "# Compute scores over range of alphas\n",
    "for alpha in alpha_space:\n",
    "\n",
    "    # Specify the alpha value to use: ridge.alpha\n",
    "    ridge.alpha = alpha\n",
    "    \n",
    "    # Perform 10-fold CV: ridge_cv_scores\n",
    "    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)\n",
    "    \n",
    "    # Append the mean of ridge_cv_scores to ridge_scores\n",
    "    ridge_scores.append(np.mean(ridge_cv_scores))\n",
    "    \n",
    "    # Append the std of ridge_cv_scores to ridge_scores_std\n",
    "    ridge_scores_std.append(np.std(ridge_cv_scores))\n",
    "\n",
    "# Display the plot\n",
    "display_plot(ridge_scores, ridge_scores_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
