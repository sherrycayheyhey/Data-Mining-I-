{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b36eaf",
   "metadata": {},
   "source": [
    "#### How Good is Your Model?\n",
    "\n",
    "accuracy (the fraction of correctly classified samples) can be used no measure model performance when working with classification, but accuracy isn't always a useful metric, for example if 99% of emails are real and 1% are spam then you could build a model that allows all emails and it would be 99% accurate but that classifier is actually doing a horrible job at detecting spam because it never actually predicts spam at all which means it's completely failing at its original purpose\n",
    "\n",
    "**class imbalance** is when one situation is more frequent, this is actually pretty common (like the spam example) and requires a more nuanced metric so that you can assess the performance of the model \n",
    "\n",
    "you could make a 2x2 confusion matrix for a binary classifier, with predicted labels across the top, the actual labels down the side, and then fill in the stuff like true positive, false positive, etc., the class of interest is usally the positive class so since you're looking for spam the positive class would probably be that one \n",
    "\n",
    "you can calculate metrics from the confusion matrix:\n",
    "- accuracy is (tp+tn)/(tp+tn+fp+fn), which would be the sum of the diagonal divided by the total sum of the matrix\n",
    "- precision (PPV, positive predictive value) is tp/(tp+tf) which would be the total number of correctly labeled spam emails divided by the total number of emails classified as spam, high precision means not many real emails were incorrectly predicted to be spam (a low false positive rate)\n",
    "- recall (sensitivity, hit rate, true positive rate) is tp/(tp+fn), F1 score is 2*((precision*recall)/(precision+recall)) and is the harmonic mean of precision and recall , high recall means that the classifier predicted most positive (spam) emails correctly\n",
    "\n",
    "by analyzing the confusion matrix and classification report, you can get a much better understanding of your classifier's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62104586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix in scikit-learn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# instantiate the classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "# fit the training data\n",
    "knn.fit(X_train, y_train)\n",
    "# predict the labels of the test set \n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "# for all metrics is sci-kit learn: the first argument is always the true label and the second is always the prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c090c4",
   "metadata": {},
   "source": [
    "#### Logistic Regression and the ROC Curve\n",
    "\n",
    "this chapter will be about adding another model to your classification arsennal: logistic regression\n",
    "even though it seems weird, logistic regression is used in classification problems, not regression problems, log reg works for binary classification (when you have 2 possible labels for the target variable), log reg produces a linear decision boundary (a line that divides the yes (1) from the no (0)\n",
    "when given one feature, log reg will output a probability (p) with respect to the target variable, if p is greater than 0.5 then the data is labeled 1 and the data will be labeled 0 if the probability is less tha 0.5\n",
    "\n",
    "by default, the logistic regression threshold is 0.5, it's not exclusive to logistic regression and could also be used for something like KNN\n",
    "\n",
    "what happens as you vary the threshold? what happens to the true positive and false positive rates as you vary the threshold? \n",
    "- when the threshold is p=0 then the model predicts 1 for all the data, this means that the true positive rate is equal to the false positive rate which is 1\n",
    "- when the threshold is p=0 then the model predicts 0 for all the data, this means that both true and false positive rates are 0\n",
    "- when the threshold is varied between these two extremes then you'll get a series of different false positive and true positive rates \n",
    "- the set of points you get when trying all possible thresholds is called the **ROC curve** (receiver operating characteristic curve) \n",
    "\n",
    "classification reports and confusion matrices are great methods to quantitatively evaluate model performance, while ROC curves provide a way to visually evaluate models\n",
    "most classifiers in scikit-learn have a .predict_proba() method which returns the probability of a given sample being in a particular class\n",
    "\n",
    "when looking at your ROC curve, you may have noticed that the y-axis (True positive rate) is also known as recall. Indeed, in addition to the ROC curve, there are other ways to visually evaluate model performance. One such way is the precision-recall curve, which is generated by plotting the precision and recall for different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43eaf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression in scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# instantiate the classifier\n",
    "logreg = LogisticRegression()\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "# fit the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "# predict on the test set\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b4597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# first argument is given by the actual labels, the second by the predicted probabilities\n",
    "# this returns an array with 2 columns, each with the probabilities for the respective target values \n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "# unpack the results into 3 variables: the false positive rate, the true positive rate, and the thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# plot the FPR and TPR using pyplot's plot function\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Logistic Regression ROC Curve')\n",
    "plt.show();\n",
    "# this used the predicted probabilites of the model, assigning a value of 1 to the observation in question, because to compute\n",
    "# the ROC we don't only want the predictions on the test set but we want the probability that our log reg model outputs before using a \n",
    "# threshold to predict the label, to do that we apply the predict_proba method as seen above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, train a logistic regression model and see if it outperforms knn \n",
    "# Import the necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n",
    "\n",
    "# Create the classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, once you have built a logistic regression model you can evaluate its performance by plotting an ROC curve\n",
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8a28a4",
   "metadata": {},
   "source": [
    "#### Area Under the ROC Curve\n",
    "\n",
    "now that you have the ROC curve, you'll want to extract a metric of interest\n",
    "\n",
    "the larger the area under the ROC curve (AUC), the better the model is! \n",
    "if you had a binary classifier that was actually just making random guesses then it would be correct about 50% of the time\n",
    "the ROC curve would be a diagonal line and the true positive rate and false positive rate would always be equal\n",
    "the areo under the ROC curve would be 0.5, so you know that if the AUC is greater than 0.5 that the model is better than random guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute AUC in scikit-learn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# instantiate the classifier\n",
    "logreg = LogisticRegression()\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "# fit the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "# predict the labels of the test set, pass the true labels and the predicted probabilities to roc auc score\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to compute AUC, compute AUC using cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# pass the estimator, the features, the target\n",
    "cv_scores = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa79256",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning\n",
    "\n",
    "now that you have a feel for how well your models are performing, let's supercharge them!!!!!!\n",
    "\n",
    "when fitting a linear regression, what you're really doing is choosing parameters for the model that fit the data the best\n",
    "- we also saw that we had to choose a value for alpha in ridge and lasso regression before fitting it \n",
    "- before fitting and predicting k-nearnest neighbors we needed to choose n neighbors\n",
    "- in logistic regression the regularization parameter is C, it controls the inverse of the regularization strength, a large C can lead to an overfit model and a small one can lead to a underfit model\n",
    "- decision trees have lots of paramaters that can be tuned like max_features, max_depth, and min_samples_leaf (so it's a great use case for RandomizedGridSearch)\n",
    "\n",
    "**hyperparameters** parameters that need to be specified before fitting a model, these are parameters that can't be explicitly learned by fitting the model  \n",
    "\n",
    "how to choose the correct hyperparameter, **hyperparameter tuning**:\n",
    "- try a bunch of different hyperparameter values\n",
    "- fit all of the separately\n",
    "- see how well each performs\n",
    "- choose the best one\n",
    "\n",
    "when fitting different values of a hyperparameter you have to use cross validation because using the train/test split alone risks overfitting the hyperparameter to the test set, next you'll see that even after tuning the hyperparameters using cross validation you want to have already split off a test set so that we can report how well our model is expected to perform on a dataset that it's never seen before \n",
    "\n",
    "**grid search cross-validation**\n",
    "- choose a grid of possible values that you want to try for the hyperparameter(s), for example if you had two hyperparameters (C and alpha) then you'd have a grid with C values of 0.1-0.5 and alpha values of 0.1 to 0.4\n",
    "- then perform k-fold cross-validation fer each point in the grid (each choice of hyperparameter or combo of hyperparameters)\n",
    "- choose the one that performed the best (the highest value) and the corresponding values are the hyperparameters you want\n",
    "\n",
    "GridSearchCV can be computationally expensive so a solution is to use RandomizedSearchCV which samples a specified number of hyperparameter settings instead of trying out everything,  RandomizedSearchCV will never outperform GridSearchCV but it's value comes from saving computation time \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9833b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search cv in scikit-learn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# specify the parameters as a dictionary with the hyperparameter names as keys (n-neighbors in KNN, or alpha in lasso regression)\n",
    "param_grid = {'n-neighbors': np.arange(1, 50)}\n",
    "# the values in the grid dictionary ore lists of the values you're trying to tune to the correct hyperparameter(s) over\n",
    "# if you specify multiple paramaters then all combinations will be tried\n",
    "\n",
    "# instantiate the classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# pass the model to grid search, the grid you want to tune over, and the number of folds you want\n",
    "knn_cv = GridSearchCv(knn, param_grid, cv=5)\n",
    "# this will return a GridSearch object that you can fit to the data and that's what actually performs the grid search inplace\n",
    "\n",
    "# get your results\n",
    "knn_cv.best_params_\n",
    "knn_cv.best_score_\n",
    "# this will tell you the hyperparameters that perform the best and the mean cv score over that fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c530c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercize example, find the C parameter for logistic regression\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a85628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercize example, random grid search with parameters for a decision tree\n",
    "# Import necessary modules\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiate a Decision Tree classifier: tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "tree_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f78748",
   "metadata": {},
   "source": [
    "#### Hold-out Set for Final Evaluation\n",
    "\n",
    "after doing all this stuff, you want to figure out how well your model can perform on never before seen data, you want to use your model to predict on some labeled data then compare the prediction to the actual labels and compute the scoring function\n",
    "\n",
    "the issue arise when you've used all of your data in cross validation and judging the performance on any of that data might not provide an accurate picture of how it'll perform on unseen data \n",
    "\n",
    "solution: \n",
    "- split data into training and hold-out sets at the very beginning\n",
    "- perform grid search cv on the training set to tune the model's hyperparameters \n",
    "- choose the best hyperparameters and evaluet on the hold-out set, this will tell you how the model is expected to perform on data it's never seen before \n",
    "\n",
    "another penalty in addition to lasso and ridge is the **elastic net** regularization, the penalty term is a linear combination of the L1 and L2 penalties (a*L1+b*L2), in scikit-learn it's represented by the \"l1_ratio) paramater, 1 is an L1 penalty and anything lower is a combination of L1 and L2\n",
    "\n",
    "the next section will be about preprocennitf techniques and how to piece together all the different stages of the machine learning process into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, hold-out set for classification\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Instantiate the logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, hold-out set for regression\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "l1_space = np.linspace(0, 1, 30)\n",
    "param_grid = {'l1_ratio': l1_space}\n",
    "\n",
    "# Instantiate the ElasticNet regressor: elastic_net\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Setup the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "gm_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and compute metrics\n",
    "y_pred = gm_cv.predict(X_test)\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))\n",
    "print(\"Tuned ElasticNet MSE: {}\".format(mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
