{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a568db3",
   "metadata": {},
   "source": [
    "#### Bagging\n",
    "\n",
    "bagging is an ensemble method that trains the same algorithm many times using different subsets sampled from the training data, bagging can be used to create a tree ensemble, the random forests algorithm can create further ensemble diversity through randomization at the level of each spilt in the trees forming ensemble \n",
    "\n",
    "**bagging** is also known as **b**ootstrap **agg**regation, in baggin the ensemble is formed by models that use the same training algorithm, these models aren't trained on the entire training set because they're trained on different subsets of the data, overall bagging is the effect of reducing the variance of individual models in the ensemble\n",
    "\n",
    "a bootstrap sample is sampling with replacement, all of these samples are then used to train n models that use the same algorithm\n",
    "\n",
    "each model outputs a prediction and then the meta model collects them and outputs a final prediction based on the problem\n",
    "\n",
    "classification uses majority voting for its final prediction, BaggingClassifier\n",
    "\n",
    "regression uses the average of the predictions made by the individual models that form the ensemble for its final prediction, BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba58e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models and utility functions\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# split the dataset into 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# instantiate a classification tree\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "# instantiate a bagging classifier that consists of 300 classification trees\n",
    "# setting n_jobs to -1 makes it so all CPU cores are used in computation\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)\n",
    "\n",
    "# fit to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "# predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# evaluate and print the test set accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc8311",
   "metadata": {},
   "source": [
    "#### Out of Bag Evaluation\n",
    "\n",
    "when you use bagging some instances may be sampled several times for one model and some instances may not be sampled at all\n",
    "for each model, on average, 63% of the training instances are sampled\n",
    "the remaining 37% that aren't sampled are the *OOB* out of bag instances, the OOB instances aren't seen by a model during training so they can be used to estimate the performance of the ensemble so that you don't need to do cross validation, this is OOB evaluation, in other words, train on the bootstrap samples and evaluate on the OOB samples, the OOB score of the bagging ensemble is the average of the OOB scores\n",
    "\n",
    "the oob score corresponds to accuracy for classifiers and r squared for regressors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7527e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOB evaluation in sklearn\n",
    "# is a tumor benign or not?\n",
    "# import models and utility functions\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# split the dataset into 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# instantiate a classification tree\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "# instantiate a bagging classifier that consists of 300 classification trees\n",
    "# use oob_score so you can get the oob accuracy of bc after training\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, oob_score=True, n_jobs=-1)\n",
    "\n",
    "# fit to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "# predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# evaluate and print the test set accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))\n",
    "\n",
    "# print the test oob accuracy\n",
    "print('OOB accuracy: {:.3f}'.format(oob_accuracy))\n",
    "# another way to do this # Evaluate OOB accuracy\n",
    "# acc_oob = bc.oob_score_\n",
    "\n",
    "# the two accuracies in this case are pretty close, oob can be used to get a good performance metric on unseen data without cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e2caf",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "\n",
    "random forests is another ensemble learning method\n",
    "\n",
    "in bagging the base estimator can be anything (like dt, lr, or even a neural network), each estimator is trained on a distinct bootstrap sample of the training set, estimators use all features for training and prediction\n",
    "\n",
    "the random forest ensemble method uses a decision tree as a base estimator, each estimator is trained on a different bootstap sample which is the same size as the training set, rf introduces even more randomization then bagging when training each of the trees (base estimators), when each tree is trained only d features can be sampled at each node without replacement (d is a number smaller than the total number of features), in scikit-learn the default d is the square root of the number of features (100 features would mean that only 10 features are sampled at each node)\n",
    "each prediction is collected by the random forests meta classifier and the final prediction is made based on the nature of the problem, classification=majority voting (RandomForestClassifier), regression=average of all the labels predicted by the base estimator (RandomForestRegressor)\n",
    "\n",
    "random forests usually achieve a lower variance than individual trees \n",
    "\n",
    "when a tree based method is trained the predictive power of a feature or its importance can be assessed, in other words tree based methods enable measurcing the importance of each feature in the prediction, in sklearn it's measured by how much the tree nodes use a particular feature (weighted average) to reduce impurity, the importance of a feature is expressed as a percentage which indicated the weight of that feature in training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667bc680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forests regressor in sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# split the dataset into 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# instantiate a random forest regressor with 400 estimators, each leaf will contain at least 12% of the data used in training\n",
    "rf = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=SEED)\n",
    "\n",
    "# fit to the training set\n",
    "rf.fit(X_train, y_train)\n",
    "# predict test set labels\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# evaluate and print the test set RMSE \n",
    "rmse_tree = MSE(y_test, y_pred)**(1/2)\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n",
    "# the RMSE shows a smaller error than the one achieved by a single regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3aa85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the importance of features as assessed by rf in sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a series of features importances\n",
    "importances_rf = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "\n",
    "# sort the importances\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "\n",
    "# make a horizontal bar plot\n",
    "sorted_importances_rf.plot(kind='barh', color='lightgreen'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd770d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
