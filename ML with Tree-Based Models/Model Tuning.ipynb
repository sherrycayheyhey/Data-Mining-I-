{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7e5004",
   "metadata": {},
   "source": [
    "#### Tuning a CART's Hyperparameters\n",
    "\n",
    "the hyperparameters of a machine learning model and the parameters that aren't learned from data, they should be set prior to fitting the model to the training set, we'll learn how to tune the hyperparameters of a tree-based model using grid search and cross validation\n",
    "\n",
    "to have better performance, the hyperparameters of a machine learning model should be tuned\n",
    "machine learning models are characterized by parameters and hyperparameters, parameters are learned from data, CART has split-point of a node, split-feature of a node, etc., hyperparameters aren't learned from data, you set them prior to training, CART has max_depth, min_samples_leaf, splitting criterion, etc.\n",
    "\n",
    "hyperparamater tuning is when you search for the set of optimal hyperparameters for the learning algorithm, the solution is to find the set of optimal hyperparameters that results in an optimal model, the optimal model yields an optimal score, the score function measures the agreement between true labels and a model's prediction, in sklearn the default is accuracy (classification), and r squared (regression), the model's generalization performance is evaluated using cv\n",
    "\n",
    "why do we tune hyperparameters? sklearn's model default parameters aren't optimal for all problems so you need to tune the hyperparamaters to obtain the best model performance\n",
    "\n",
    "there are lots of approaches to hyperparameter tuning like grid search, random search, bayesian optimization, genetic algorithms, etc.\n",
    "\n",
    "grid search cross validation:\n",
    "- manually set a grid of discrete hyperparameter values\n",
    "- pick a metric for scoring model performance\n",
    "- search exhaustively through the grid\n",
    "- for each set of hyperparameters you evaluate each model's cv score\n",
    "- the optimal hyperparameters are those for which the model achieves the best cross-validation score\n",
    "\n",
    "grid search suffers from the curse of dimensionality, the larger the grid the longer it takes to find the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting the hyperparameters of a CART in sklearn\n",
    "import sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# instantiate a dtc\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "# print out a dictionary, the keys are the hyperparameter names, \n",
    "print(dt.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1393a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune dt on the cancer dataset, it's already loaded and split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define a dictionary with the names of the hyperparameters you want to tune as the keys and lists of hyperparameter values as values\n",
    "params_dt = {\n",
    "             'max_depth': [3, 4, 5, 6], \n",
    "             'min_samples_leaf': [0.04, 0.06, 0.08], \n",
    "             'max_features': [0.2, 0.4, 0.6, 0.8]\n",
    "            }\n",
    "\n",
    "# instantiate a grid search cv object\n",
    "grid_dt = GridSearchCV(estimator=dt, \n",
    "                       param_grid=params_dt, \n",
    "                       scoring='accuracy',\n",
    "                       cv=10, \n",
    "                       n-jobs=-1)\n",
    "\n",
    "# fit to the training set\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "# extract the best hyperparameters \n",
    "best_hyperparams = grid_dt.best_params_\n",
    "print('Best hyperparameters:\\n', best_hyperparams)\n",
    "\n",
    "# extract the best cross validation accuracy score\n",
    "best_CV_score = grid_dt.best_score_\n",
    "print('Best CV accuracy'.format(best_CV_score))\n",
    "\n",
    "# extract the best model\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# this model is fitted on the whole training set because the refit parameter of the GridSearchCV is set to True by default\n",
    "\n",
    "# evaluate test set accuracy\n",
    "test_acc = best_model.score(X_test, y_test)\n",
    "print(\"Test set accuracy of best model: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38257043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, tune the hyperparameters of a classification tree\n",
    "# the dataset is imbalanced so you'll use the ROC AUC score as a metric instead of accuracy\n",
    "# set scoring to 'roc_auc'\n",
    "#  evaluate the test set ROC AUC score of grid_dt's optimal model.\n",
    "# In order to do so, you will first determine the probability of obtaining the positive label for each test set observation\n",
    "# you can use the methodpredict_proba() of an sklearn classifier to compute a 2D array containing the probabilities of the \n",
    "# negative and positive class-labels respectively along columns\n",
    "# do all the same stuff as above \n",
    "\n",
    "# Import roc_auc_score from sklearn.metrics \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40253c",
   "metadata": {},
   "source": [
    "#### Tuning an RF's Hyperparameters\n",
    "\n",
    "we'll now tune the hyperparameters of Random Forests which is an ensemble method\n",
    "\n",
    "in addition to the hyperparameters of the CARTs forming random forests, the ensemble itself is characterized by other hyperparameters like the number of estimators, whether it uses bootstrapping, etc.\n",
    "\n",
    "hyperparameter tuning is computationally expensive and might only lead to a very slight improvement in some situations :(\n",
    "because of this, you should weight the impact of tuning on the pipeline of your data analysis project as a whole so that you can understand if its worth pursuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf3153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect RF hyperparameters in sklear\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# instantiate a random forests regressor\n",
    "rf = RandomForestRegressor(random_state=SEED)\n",
    "\n",
    "# inspect rf's hyperparameters\n",
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform grid-search cross-validation on the dataset which is already loaded and split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define a dictionary containing the grid of hyperparameters\n",
    "params_rf = {\n",
    "             'n_estimators': [300, 400, 500], \n",
    "             'max_depth': [4, 6, 8],\n",
    "             'min_samples_leaf': [0.1, 0.2], \n",
    "             'max_features': ['log2', 'sqrt']\n",
    "            }\n",
    "\n",
    "# instantiate a grid search cv object with 3 fold cv\n",
    "# verbose controls verbosity, the higher the value the more messages are printed during fitting\n",
    "grid_rf = GridSearchCV(estimator=rf, \n",
    "                       param_grid=params_rf, \n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       verbose=1, \n",
    "                       n-jobs=-1)\n",
    "\n",
    "# fit to the training set\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# extract the best hyperparameters \n",
    "best_hyperparams = grid_rf.best_params_\n",
    "print('Best hyperparameters:\\n', best_hyperparams)\n",
    "\n",
    "# extract the best model\n",
    "best_model = grid_rf.best_estimator_\n",
    "# predict on test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "# evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
