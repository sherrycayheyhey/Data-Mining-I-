{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db59bdbf",
   "metadata": {},
   "source": [
    "#### Adaboost\n",
    "\n",
    "boosting is an ensemble method where several models are trained sequentially with each model learning from the errors of its predecessors, two methods of boosting are AdaBoost and Gradient Boosting, boosting combines a bunch of weak learners (slightly better than random guessing) to form a strong learner \n",
    "\n",
    "a decision stump is a CART with a maximum depth of 1 and is an example of a weak learner \n",
    "\n",
    "**adaboost** is adaptive boosting, each predictor pays more attention to the instances wrongly predicted by its predecessor by constantly changing the weights of training instances, each predictor is assingned a coefficient **alpha**, alpha depends on the predictor's training error (it weighs its contribution in the ensemble's final prediction), the learning rate **eta** is a number between 0 and 1 and its used to strink the coefficient alpha of a trained predictor, there's a tradeoff between eta and the number of estimators (a smaller eta value should be compensated by a greater number of estimators)\n",
    "for a classification prediction weighted majority voting is used (AdaBoostClassifier), for a regression prediction weighted average is used (AdaBoostRegressor)\n",
    "individual predictors don't need to be CARTs but CARTs are used most of the time in boosting because of their high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de54573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classification in sklearn\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# split the dataset into 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# instantiate a classification tree, a stump hehe :)\n",
    "dt = DecisionTreeClassifier(max_depth=1, random_state=SEED)\n",
    "\n",
    "# instantiate an AdaBoost classifier with 100 decision stumps\n",
    "adb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=100)\n",
    "\n",
    "# fit to the training set\n",
    "adb_clf.fit(X_train, y_train)\n",
    "# predict the test set probabilities of positive class\n",
    "y_pred_proba = adb_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# you can now evaluate the roc auc score of the test set\n",
    "adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# print the result\n",
    "print('ROC AUC score: {:.2f}'.format(adb_clf_roc_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0187483",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "\n",
    "in gradient boosing each predictor is the ensemble corrects its predecessor's error (sequential correction of predecessor's errors), unlike AdaBoost the weights of the training instances are not tweaked, instead each predictor is trained using the residual errors of its predecessor as labels, gradient boosted trees use a CART as the base learner \n",
    "\n",
    "**shrinkage** is an important parameter used in training gradient boosted trees, shrinkage refers to the fact that the prediction of each tree in the ensemble is shrinked after it's multiplied by a learning rate **eta** which is a number between 0 and 1, just like with AdaBoost, there's a tradeoff between eta and the number of estimators (decreasing the learning rate needs to be compensated by increasing the number of estimators), the class for a gradient boosting regressor is sklearn is GradientBoosingRegressor, a similar algorithm is used for classification problems, the class implements gradient boosted classification in sklearn is GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2aec110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting in sklearn\n",
    "# predict the mpg consumption of cars \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# split the dataset into 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# instantiate a GBR consisting of 300 decision stumps\n",
    "gbt = GradientBoostingRegressor(n_estimators=300, max_depth=1, random_state=SEED)\n",
    "\n",
    "# fit to the training set\n",
    "gbt.fit(X_train, y_train)\n",
    "# predict the test set labels\n",
    "y_pred = gbt.predict(X_test)\n",
    "\n",
    "# evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac2399",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Boosting (SGB)\n",
    "\n",
    "gradient boosting cons:\n",
    "- it involves an exhaustive search procedure\n",
    "- each tree (CART) in the ensemble is trained to find the best split-points and the best features\n",
    "- this procedure may lead to CARTs that use the same split-points and possibly the same features\n",
    "\n",
    "to mitigate these cons you can use the stochastic gradient boosting algorithm, each cart/tree is trained on a random subset of rows of the training data, the subset is sampled without replacement and are 40-80% of the training set, at each node features are sampled (without replacement) when choosing split points, this creates further diversity in the ensemble and has a net effect of adding further variance to the ensemble of trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca4b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosting in sklearn\n",
    "# predict the mpg consumption of cars \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# split the dataset into 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# instantiate a stochastic GradientBoostingRegressor consisting of 300 decision stumps\n",
    "# subsample is % of data each tree uses for training, max features is % of available features to perform the best split\n",
    "sgbt = GradientBoostingRegressor(max_depth=1, \n",
    "                                 subsample=0.8, \n",
    "                                 max_features=0.2, \n",
    "                                 n_estimators=300, \n",
    "                                 random_state=SEED)\n",
    "\n",
    "# fit to the training set\n",
    "sgbt.fit(X_train, y_train)\n",
    "# predict the test set labels\n",
    "y_pred = sgbt.predict(X_test)\n",
    "\n",
    "# compute the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
