{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bca62c",
   "metadata": {},
   "source": [
    "#### Decision Tree for Classification\n",
    "\n",
    "tree models have high flexibility and can capture complex non-linear relationships but they're prone to memorizing the noise present in a dataset, one solution is to aggregate the predictions of trees that are trained differently which is called ensemble method\n",
    "\n",
    "CART (classification and regression trees) are a set of supervised learning models used for problems involving classification and regression\n",
    "this chapter talks about the CART algorithm\n",
    "\n",
    "a **classification tree** learns a sequence of if-else questions about individual features when given a labeled dataset, its objective is to infel class labels, trees are able to capture non-linear relationships between features and labels (whereas linear models can't), trees don't require features to be on the same scale so you don't have to do something like standardization, an example of a problem is predicting whether a tumor is malignant or benign using only 2 features\n",
    "\n",
    "when a classification tree is trained on a dataset like this the tree will learn a squence of if-else questions which each question involving one feature and one split-point\n",
    "\n",
    "the maximum number of branches from the top from an extreme-end is called the maximum depth, a tree with 2 questions is 2 (3 levels)\n",
    "\n",
    "a classification model divides the feature-space into regions, **decision regions** are a region in the feature space where all instances in one region are assigned to one and only one class label, decision regions are separated by surfaces called **decision boundaries**, decision boundaries are like the dividing line and then the regions are the sections that line makes, in a linear model like logistic regression the regions are a single straight line decision boundary but with decision trees the decision regions are divided into rectangular regions because only one feature is involved at each split made by the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification-tree in scikit-learn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# split the dataset into 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "\n",
    "# instantiate the decision tree with a maximum depth of 2\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "# predict the test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# evaluate the test-set accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cf1d8d",
   "metadata": {},
   "source": [
    "#### Classification Tree Learning\n",
    "\n",
    "a **decision tree** is a data structure consisting of a hierarchy individual units of nodes\n",
    "a **node** is a question or prediction, there are 3 kinds of nodes:\n",
    "- the **root** is where the decision tree starts growing, it has no parent node, its question gives rise to 2 children nodes\n",
    "- an **internal node** has 1 parent and its question gives rise to 2 children nodes\n",
    "- a **leaf** has 1 parent node and 0 children (no questions), it's where a prediction is made\n",
    "\n",
    "a decision tree creates the purest leaf possible, the nodes of a classification tree are grown recursively (the internal node or leaf depends on the state of its predecessors, fto create a pure leaf, at each node a tree asks a question involving one feature (f) and a split-point (sp), but how does it know which feature and which split-point to pick?\n",
    "\n",
    "the tree answers this question by maximizing information gain, the tree knows that every node contains information and it aims at maximizing the **information gain** (IG) obtained after each split\n",
    "\n",
    "another question is what criterion is used to measure the impurity of a node? two of the options are:\n",
    "- gini index\n",
    "- entropy\n",
    "most of the time the gini index and entropy lead to the same results but the gini index is slightly faster to compute and is the default criterion used in the DecisionTreeClassifier model of sckit-learn\n",
    "\n",
    "\n",
    "classification-tree learning, how the tree learns:\n",
    "- when an unconstrained tree is trained, the nodes are grown recursively (a node exists based on the state of its predecessors)\n",
    "- at each non-leaf node, the data is split based on feature f and split-point sp to maximize IG\n",
    "- if the information gained obtained by splittg a node is null then the node is declared a leaf, IG(node)=0 make it a leaf\n",
    "- if the tree is constrained, like a max depth of 2, then all nodes having a depth of 2 will be declared leafs even if the info obtained by such nodes is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffce46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could now update the previous code with a criterion using the gini-index\n",
    "dt = DecisionTreeClassifier(criterion='gini', random_state=1)\n",
    "# the rest of the code is the same too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, train a classification tree using entropy as an information criterion\n",
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise example, compare entropy to gini index\n",
    "# Import accuracy_score from sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use dt_entropy to predict test set labels\n",
    "y_pred = dt_entropy.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy_entropy\n",
    "print(f'Accuracy achieved by using entropy: {accuracy_entropy:.3f}')\n",
    "\n",
    "# Print accuracy_gini\n",
    "print(f'Accuracy achieved by using the gini index: {accuracy_gini:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad585df",
   "metadata": {},
   "source": [
    "#### Decision Tree for Regression\n",
    "\n",
    "time to learnn how to train a decision tree for a regression problem!!\n",
    "\n",
    "remember: the target variable in regression is continuous, the output of your model is a real value\n",
    "\n",
    "you could make a scatterplot of mpg versus the displacement of a car, you'd see that the mpg consumption decreases nonlinearly (kinda banana-shaped) with displacement, linear models like linear regression wouldn't be able to capture a non-linear trend like this \n",
    "\n",
    "when a regression tree is trained on a dataset, the impurity of a node is measured using the mean squared error of the targets in that node, this means the regression tree tries to find the splits that produce leafs so that in each leaf the target values are, on average, the closest possible to the mean value of the labels in that particular leaf \n",
    "the RMSE of a model measures, on average, how much the model's predictions differ from the actual labels\n",
    "\n",
    "as a new instance traverses the tree and reaches a certain leaf, its target variable y is computed as the average of the target variables contained in that leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression tree in scikit-learn\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# split the dataset into 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=3)\n",
    "\n",
    "# instantiate the decision tree regressor\n",
    "# min_sample_leaf if for a stopping condition in which each leaf has to contain at least 10% of the training data\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.1, random_state=3)\n",
    "\n",
    "# fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "# predict test-set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# obtain the root mean squared error of the model on the test set \n",
    "# evaluate the mean squared error\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "# you could also compute the 10-folds CV RMSE by squaring the root of the average MSE\n",
    "# RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "\n",
    "# raise the obtained value to the power 1/2\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "print(rmse_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f8a96",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
